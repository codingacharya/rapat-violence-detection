# -*- coding: utf-8 -*-
"""p1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZdPDQfiad6a6r7riil3vKW5B7dn3v3dY
"""

from google.colab import drive
import os

drive.mount('/content/drive')

DATASET_DIR = "/content/drive/MyDrive/data/processed"

print("‚úÖ Dataset folder exists:", os.path.exists(DATASET_DIR))
print("Subfolders:", len(os.listdir(DATASET_DIR)))

# Show a few samples
for i, folder in enumerate(os.listdir(DATASET_DIR)[:3]):
    frames_path = os.path.join(DATASET_DIR, folder, "frames")
    if os.path.exists(frames_path):
        print(f"{i+1}. {folder} ‚Üí {len(os.listdir(frames_path))} frames")

!pip install torch torchvision timm opencv-python librosa matplotlib tqdm

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import torch

class RPATDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.video_folders = [
            os.path.join(root_dir, d) for d in os.listdir(root_dir)
            if os.path.isdir(os.path.join(root_dir, d))
        ]
        self.transform = transform

    def __len__(self):
        return len(self.video_folders)

    def __getitem__(self, idx):
        folder = self.video_folders[idx]
        frames_dir = os.path.join(folder, "frames")
        frame_files = sorted(os.listdir(frames_dir))
        mid_frame = frame_files[len(frame_files)//2]
        image = Image.open(os.path.join(frames_dir, mid_frame)).convert("RGB")

        # label.txt must contain ‚Äúviolent‚Äù or ‚Äúnon_violent‚Äù
        with open(os.path.join(folder, "label.txt")) as f:
            text = f.read().strip().lower()
        label = 1 if "violent" in text else 0

        if self.transform:
            image = self.transform(image)

        return image, torch.tensor(label)

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

dataset = RPATDataset(DATASET_DIR, transform=transform)
train_loader = DataLoader(dataset, batch_size=8, shuffle=True)

print("‚úÖ Total videos:", len(dataset))
for imgs, labels in train_loader:
    print("Sample batch:", imgs.shape, labels)
    break

import torch.nn as nn
import timm

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", device)

# Pretrained Vision Transformer backbone
model = timm.create_model("vit_base_patch16_224", pretrained=True)
model.head = nn.Linear(model.head.in_features, 2)   # binary classifier
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

print("‚úÖ Model ready.")

!pip install opencv-python

from tqdm import tqdm

epochs = 3
for epoch in range(epochs):
    model.train()
    running_loss, correct, total = 0, 0, 0

    for imgs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
        imgs, labels = imgs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        preds = outputs.argmax(1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    acc = correct / total
    print(f"Epoch {epoch+1}: Loss={running_loss/len(train_loader):.4f} | Acc={acc:.3f}")

from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np

class RPATMotionDataset(Dataset):
    def __init__(self, root_dir, transform=None, num_frames=3):
        self.root_dir = root_dir
        self.video_folders = [
            os.path.join(root_dir, d) for d in os.listdir(root_dir)
            if os.path.isdir(os.path.join(root_dir, d))
        ]
        self.transform = transform
        self.num_frames = num_frames

    def __len__(self):
        return len(self.video_folders)

    def __getitem__(self, idx):
        folder = self.video_folders[idx]
        frames_dir = os.path.join(folder, "frames")
        frame_files = sorted(os.listdir(frames_dir))
        mid = len(frame_files)//2

        # pick surrounding frames
        start = max(0, mid - self.num_frames//2)
        end = min(len(frame_files), start + self.num_frames)
        selected = frame_files[start:end]

        # load frames
        frames = [cv2.cvtColor(cv2.imread(os.path.join(frames_dir, f)), cv2.COLOR_BGR2RGB) for f in selected]

        # compute frame difference
        diffs = []
        for i in range(len(frames)-1):
            diff = cv2.absdiff(frames[i+1], frames[i])
            diff = cv2.cvtColor(diff, cv2.COLOR_RGB2GRAY)
            diffs.append(diff)

        motion_map = np.mean(np.stack(diffs, axis=0), axis=0)  # avg motion intensity
        motion_map = np.expand_dims(motion_map, axis=-1).repeat(3, axis=-1)  # 3-channel

        # choose mid-frame as RGB image
        image = frames[len(frames)//2]
        image = Image.fromarray(image)
        motion_image = Image.fromarray(motion_map.astype(np.uint8))

        with open(os.path.join(folder, "label.txt")) as f:
            label = 1 if "violent" in f.read().lower() else 0

        if self.transform:
            image = self.transform(image)
            motion_image = self.transform(motion_image)

        # concatenate: [RGB + Motion] ‚Üí 6 channels
        combined = torch.cat((image, motion_image), dim=0)
        return combined, torch.tensor(label)

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

motion_dataset = RPATMotionDataset(DATASET_DIR, transform=transform)
motion_loader = DataLoader(motion_dataset, batch_size=4, shuffle=True)

imgs, labels = next(iter(motion_loader))
print("Input shape:", imgs.shape)  # expect (B, 6, 224, 224)

import torch.nn.functional as F

class ViT_MEPE(nn.Module):
    def __init__(self, pretrained_name="vit_base_patch16_224", num_classes=2):
        super().__init__()
        self.backbone = timm.create_model(pretrained_name, pretrained=True)
        in_ch = 6  # 3 (RGB) + 3 (Motion)
        self.backbone.patch_embed.proj = nn.Conv2d(
            in_ch,
            self.backbone.patch_embed.proj.out_channels,
            kernel_size=self.backbone.patch_embed.proj.kernel_size,
            stride=self.backbone.patch_embed.proj.stride,
        )
        self.backbone.head = nn.Linear(self.backbone.head.in_features, num_classes)

    def forward(self, x):
        return self.backbone(x)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = ViT_MEPE().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

print("‚úÖ MEPE-enabled ViT ready.")

from tqdm import tqdm

epochs = 3
for epoch in range(epochs):
    model.train()
    total, correct, running_loss = 0, 0, 0

    for imgs, labels in tqdm(motion_loader, desc=f"Epoch {epoch+1}/{epochs}"):
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()

        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        preds = outputs.argmax(1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    acc = correct / total
    print(f"Epoch {epoch+1}: Loss={running_loss/len(motion_loader):.4f} | Acc={acc:.3f}")

!pip install mediapipe opencv-python

import mediapipe as mp
import numpy as np
import cv2
from PIL import Image

mp_pose = mp.solutions.pose

def get_pose_mask(image, size=(224, 224)):
    image_rgb = np.array(image)
    h, w, _ = image_rgb.shape
    mask = np.zeros((h, w), dtype=np.uint8)

    with mp_pose.Pose(static_image_mode=True) as pose:
        results = pose.process(image_rgb)
        if results.pose_landmarks:
            for lm in results.pose_landmarks.landmark:
                cx, cy = int(lm.x * w), int(lm.y * h)
                cv2.circle(mask, (cx, cy), 8, 255, -1)

    mask = cv2.GaussianBlur(mask, (15,15), 0)
    mask = cv2.resize(mask, size)
    mask = mask / 255.0
    return mask

import os
from PIL import Image
import matplotlib.pyplot as plt

# Path to your processed dataset
DATASET_DIR = "/content/drive/MyDrive/data/processed"

# Pick a random sample folder manually
sample_folder = os.path.join(DATASET_DIR, os.listdir(DATASET_DIR)[0])
frames_dir = os.path.join(sample_folder, "frames")

# Pick a middle frame
frame_files = sorted(os.listdir(frames_dir))
mid_frame = frame_files[len(frame_files)//2]

# Load and visualize
image_path = os.path.join(frames_dir, mid_frame)
image = Image.open(image_path).convert("RGB")

# Generate pose mask
mask = get_pose_mask(image)

# Show the results
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.imshow(image)
plt.title("Original Frame")

plt.subplot(1,2,2)
plt.imshow(mask, cmap='hot')
plt.title("Pose Attention Mask")
plt.show()

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import torch
import cv2
import numpy as np
import os

class RPATPoseDataset(Dataset):
    def __init__(self, root_dir, transform=None, num_frames=3):
        self.root_dir = root_dir
        self.video_folders = [
            os.path.join(root_dir, d) for d in os.listdir(root_dir)
            if os.path.isdir(os.path.join(root_dir, d))
        ]
        self.transform = transform
        self.num_frames = num_frames

    def __len__(self):
        return len(self.video_folders)

    def __getitem__(self, idx):
        folder = self.video_folders[idx]
        frames_dir = os.path.join(folder, "frames")
        frame_files = sorted(os.listdir(frames_dir))
        mid = len(frame_files)//2
        image_path = os.path.join(frames_dir, frame_files[mid])

        # --- Load RGB + Motion ---
        img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
        motion_map = cv2.absdiff(img, cv2.GaussianBlur(img, (9,9), 0))
        motion_map = Image.fromarray(motion_map)

        # --- Pose mask ---
        pose_mask = get_pose_mask(Image.fromarray(img))
        pose_mask = np.expand_dims(pose_mask, axis=-1).repeat(3, axis=-1)
        pose_mask = Image.fromarray((pose_mask*255).astype(np.uint8))

        with open(os.path.join(folder, "label.txt")) as f:
            label = 1 if "violent" in f.read().lower() else 0

        if self.transform:
            img = self.transform(Image.fromarray(img))
            motion_map = self.transform(motion_map)
            pose_mask = self.transform(pose_mask)

        # Concatenate: RGB + Motion + Pose  (9 channels)
        combined = torch.cat((img, motion_map, pose_mask), dim=0)
        return combined, torch.tensor(label)

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

pose_dataset = RPATPoseDataset("/content/drive/MyDrive/data/processed", transform=transform)
pose_loader = DataLoader(pose_dataset, batch_size=4, shuffle=True)

imgs, labels = next(iter(pose_loader))
print("Batch shape:", imgs.shape)

import torch
import torch.nn as nn
import torch.nn.functional as F

class RelationalMemory(nn.Module):
    def __init__(self, mem_slots=4, head_size=64, input_size=512, num_heads=4):
        super(RelationalMemory, self).__init__()
        self.mem_slots = mem_slots
        self.head_size = head_size
        self.num_heads = num_heads
        self.d_model = head_size * num_heads

        # Initial memory (learnable)
        self.initial_memory = nn.Parameter(torch.randn(mem_slots, self.d_model))

        # Attention layers for memory updates
        self.query_proj = nn.Linear(self.d_model, self.d_model)
        self.key_proj = nn.Linear(input_size, self.d_model)
        self.value_proj = nn.Linear(input_size, self.d_model)
        self.output_proj = nn.Linear(self.d_model, self.d_model)

    def forward(self, x, memory=None):
        """
        x: (batch, seq_len, input_size)
        memory: (batch, mem_slots, d_model)
        """
        batch_size = x.size(0)
        seq_len = x.size(1)

        if memory is None:
            memory = self.initial_memory.unsqueeze(0).repeat(batch_size, 1, 1)

        # Project queries, keys, values
        Q = self.query_proj(memory)
        K = self.key_proj(x)
        V = self.value_proj(x)

        # Multi-head attention
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_size ** 0.5)
        attn_weights = F.softmax(attn_scores, dim=-1)
        context = torch.matmul(attn_weights, V)

        # Update memory
        updated_memory = self.output_proj(context + memory)
        return updated_memory

from torch.nn import TransformerEncoder, TransformerEncoderLayer

class RPATTransformer(nn.Module):
    def __init__(self, input_dim=512, num_classes=2):
        super(RPATTransformer, self).__init__()
        encoder_layer = TransformerEncoderLayer(d_model=input_dim, nhead=8, dim_feedforward=1024)
        self.transformer = TransformerEncoder(encoder_layer, num_layers=4)

        self.rel_memory = RelationalMemory(input_size=input_dim)
        self.fc = nn.Linear(input_dim, num_classes)

    def forward(self, x):
        """
        x: (batch, seq_len, input_dim)
        """
        memory = None
        outputs = []
        for t in range(x.size(1)):
            frame_feat = x[:, t, :].unsqueeze(1)
            enc_out = self.transformer(frame_feat)
            memory = self.rel_memory(enc_out, memory)
            outputs.append(memory.mean(dim=1))

        final_feat = torch.stack(outputs, dim=1).mean(dim=1)
        logits = self.fc(final_feat)
        return logits

import torch
import torch.nn as nn
import torch.nn.functional as F

class RelationalMemory(nn.Module):
    def __init__(self, mem_slots=4, head_size=64, input_size=512, num_heads=4):
        super(RelationalMemory, self).__init__()
        self.mem_slots = mem_slots
        self.head_size = head_size
        self.num_heads = num_heads
        self.d_model = head_size * num_heads

        # Learnable initial memory
        self.initial_memory = nn.Parameter(torch.randn(mem_slots, self.d_model))

        # Linear projections
        self.query_proj = nn.Linear(self.d_model, self.d_model)
        self.key_proj = nn.Linear(input_size, self.d_model)
        self.value_proj = nn.Linear(input_size, self.d_model)
        self.output_proj = nn.Linear(self.d_model, self.d_model)

    def forward(self, x, memory=None):
        batch_size = x.size(0)

        if memory is None:
            memory = self.initial_memory.unsqueeze(0).repeat(batch_size, 1, 1)

        Q = self.query_proj(memory)
        K = self.key_proj(x)
        V = self.value_proj(x)

        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_size ** 0.5)
        attn_weights = F.softmax(attn_scores, dim=-1)
        context = torch.matmul(attn_weights, V)

        updated_memory = self.output_proj(context + memory)
        return updated_memory

from torch.nn import TransformerEncoder, TransformerEncoderLayer

class RPATTransformer(nn.Module):
    def __init__(self, input_dim=512, num_classes=2):
        super(RPATTransformer, self).__init__()
        encoder_layer = TransformerEncoderLayer(d_model=input_dim, nhead=8, dim_feedforward=1024)
        self.transformer = TransformerEncoder(encoder_layer, num_layers=4)
        self.rel_memory = RelationalMemory(input_size=input_dim)
        self.fc = nn.Linear(input_dim, num_classes)

    def forward(self, x):
        """
        x: (batch, seq_len, input_dim)
        """
        memory = None
        outputs = []
        for t in range(x.size(1)):
            frame_feat = x[:, t, :].unsqueeze(1)
            enc_out = self.transformer(frame_feat)
            memory = self.rel_memory(enc_out, memory)
            outputs.append(memory.mean(dim=1))

        final_feat = torch.stack(outputs, dim=1).mean(dim=1)
        logits = self.fc(final_feat)
        return logits

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = RPATTransformer(input_dim=512, num_classes=2).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import torch
import cv2
import numpy as np
import os

def get_pose_mask(image):
    """
    Dummy pose mask for now (can replace later with MediaPipe or HRNet output).
    It highlights central human regions roughly.
    """
    img = np.array(image)
    h, w, _ = img.shape
    mask = np.zeros((h, w), dtype=np.float32)
    cy, cx = h // 2, w // 2
    mask[cy - h//4 : cy + h//4, cx - w//6 : cx + w//6] = 1.0
    return mask

class RPATPoseDataset(Dataset):
    def __init__(self, root_dir, transform=None, num_frames=3):
        self.root_dir = root_dir
        self.video_folders = [
            os.path.join(root_dir, d) for d in os.listdir(root_dir)
            if os.path.isdir(os.path.join(root_dir, d))
        ]
        self.transform = transform
        self.num_frames = num_frames

    def __len__(self):
        return len(self.video_folders)

    def __getitem__(self, idx):
        folder = self.video_folders[idx]
        frames_dir = os.path.join(folder, "frames")
        frame_files = sorted(os.listdir(frames_dir))
        mid = len(frame_files)//2
        image_path = os.path.join(frames_dir, frame_files[mid])

        # --- Load RGB + Motion ---
        img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
        motion_map = cv2.absdiff(img, cv2.GaussianBlur(img, (9,9), 0))
        motion_map = Image.fromarray(motion_map)

        # --- Pose mask ---
        pose_mask = get_pose_mask(Image.fromarray(img))
        pose_mask = np.expand_dims(pose_mask, axis=-1).repeat(3, axis=-1)
        pose_mask = Image.fromarray((pose_mask*255).astype(np.uint8))

        # --- Label ---
        label_path = os.path.join(folder, "label.txt")
        label = 0
        if os.path.exists(label_path):
            with open(label_path) as f:
                label = 1 if "violent" in f.read().lower() else 0

        # --- Transform ---
        if self.transform:
            img = self.transform(Image.fromarray(img))
            motion_map = self.transform(motion_map)
            pose_mask = self.transform(pose_mask)

        # Combine channels: RGB + Motion + Pose (9 total)
        combined = torch.cat((img, motion_map, pose_mask), dim=0)
        return combined, torch.tensor(label)

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive

!ls /content/drive/MyDrive/data

DATASET_DIR = "/content/drive/MyDrive/data/processed"

DATASET_DIR = "/content/drive/MyDrive/data"

DATASET_DIR = "/content/drive/MyDrive/data/videos/processed"

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive

!ls /content/drive/MyDrive/data

!ls /content/drive/MyDrive

!ls /content/drive/MyDrive/data

DATASET_DIR = "..."

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

DATASET_DIR = "/content/drive/MyDrive/data/processed"

pose_dataset = RPATPoseDataset(DATASET_DIR, transform=transform)
pose_loader = DataLoader(pose_dataset, batch_size=4, shuffle=True)

print("Total samples:", len(pose_dataset))

batch = next(iter(pose_loader))
inputs, labels = batch
print("Input shape:", inputs.shape)

model = RPATTransformer(input_dim=256, num_classes=2).to(device)

import torch.nn as nn
import torchvision.models as models

# Pretrained ResNet18 to extract 512-d features from frames
class FrameFeatureExtractor(nn.Module):
    def __init__(self):
        super(FrameFeatureExtractor, self).__init__()
        base_model = models.resnet18(pretrained=True)
        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # remove final FC
        self.out_dim = 512

    def forward(self, x):
        # x: [batch, channels, H, W]
        features = self.feature_extractor(x)
        features = features.view(features.size(0), -1)
        return features

feature_extractor = FrameFeatureExtractor().to(device)
feature_extractor.eval()  # No gradient updates during training

# inputs: [B, T, H, W]
inputs = inputs.unsqueeze(2).repeat(1, 1, 3, 1, 1)  # Make it [B, T, 3, H, W]

model = RPATTransformer(input_dim=512, num_classes=2)

with torch.no_grad():
    sample = torch.randn(1, 3, 224, 224).to(device)
    feat = feature_extractor(sample)
    print("Feature shape:", feat.shape)

model = RPATTransformer(input_dim=512, num_classes=2).to(device)

print("Feature batch shape:", features.shape)  # should be [B, T, 256] or [B, T, 512]

SAVE_PATH = "/content/drive/MyDrive/rpat_model.pth"
torch.save(model.state_dict(), SAVE_PATH)
print(f"‚úÖ Model saved successfully at: {SAVE_PATH}")

!ls /content/drive/MyDrive/data

import os, shutil, random

root_dir = "/content/drive/MyDrive/data"
train_dir = os.path.join(root_dir, "train")
test_dir = os.path.join(root_dir, "test")

# Create train and test directories
os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Get list of .avi files only (skip "audio" folder)
video_files = [f for f in os.listdir(root_dir) if f.endswith(".avi")]

# Shuffle for randomness
random.shuffle(video_files)

# 80% train, 20% test
split_idx = int(0.8 * len(video_files))
train_files = video_files[:split_idx]
test_files = video_files[split_idx:]

# Move files into train and test folders
for f in train_files:
    shutil.move(os.path.join(root_dir, f), os.path.join(train_dir, f))
for f in test_files:
    shutil.move(os.path.join(root_dir, f), os.path.join(test_dir, f))

print(f"‚úÖ Split complete: {len(train_files)} train videos, {len(test_files)} test videos.")

!ls /content/drive/MyDrive/data/train | head
!ls /content/drive/MyDrive/data/test | head

test_dataset = RPATPoseDataset("/content/drive/MyDrive/data/test", transform=transform)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

!ls -R /content/drive/MyDrive/data/test | head -20

!ls /content/drive/MyDrive/data/test

import torch
from torch.utils.data import Dataset
import os
import cv2
import numpy as np
from PIL import Image

class RPATPoseDataset(Dataset):
    def __init__(self, root_dir, transform=None, num_frames=9):
        self.root_dir = root_dir
        self.video_files = [
            os.path.join(root_dir, f)
            for f in os.listdir(root_dir)
            if f.endswith('.avi')
        ]
        self.transform = transform
        self.num_frames = num_frames

    def __len__(self):
        return len(self.video_files)

    def __getitem__(self, idx):
        video_path = self.video_files[idx]
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        if total_frames == 0:
            cap.release()
            raise ValueError(f"‚ö†Ô∏è Video {video_path} has no frames")

        frame_idxs = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)
        frames = []

        for i in frame_idxs:
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame = Image.fromarray(frame)
                if self.transform:
                    frame = self.transform(frame)
                frames.append(frame)

        cap.release()

        frames = torch.stack(frames)
        label = 1 if "fi" in video_path.lower() else 0  # 1 = violent, 0 = non-violent
        return frames, label

DATASET_DIR = "/content/drive/MyDrive/data/test"

test_dataset = RPATPoseDataset(DATASET_DIR, transform=transform)
print("‚úÖ Test samples found:", len(test_dataset))

from torch.utils.data import DataLoader
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

for inputs, labels in test_loader:
    print(inputs.shape)
    break

import torchvision.models as models
import torch.nn as nn

# Use ResNet-50 as feature extractor
resnet = models.resnet50(pretrained=True)
feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # remove classification head
feature_extractor.eval()
feature_extractor.to(device)

projection = nn.Linear(2048, 512).to(device)

model = RPATTransformer(input_dim=512, num_classes=2)

nn.Linear(512, 2)

import torch
import torch.nn as nn

class RPATTransformer(nn.Module):
    def __init__(self, input_dim=512, num_classes=2, hidden_dim=256, num_heads=8, num_layers=2):
        super(RPATTransformer, self).__init__()

        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=input_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)

        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        # x shape: [batch, sequence, input_dim]
        x = self.transformer(x)
        x = x.mean(dim=1)            # temporal pooling
        x = self.relu(self.fc1(x))
        out = self.fc2(x)
        return out

model = RPATTransformer(input_dim=512, hidden_dim=256, num_classes=2).to(device)

# Commented out IPython magic to ensure Python compatibility.
# %reset -f

import os
import torch
from torch.utils.data import Dataset
from PIL import Image
from torchvision import transforms

class RPATPoseDataset(Dataset):
    def __init__(self, root_dir, transform=None, num_frames=9):
        self.root_dir = root_dir
        self.transform = transform
        self.num_frames = num_frames
        self.video_folders = [
            os.path.join(root_dir, d)
            for d in os.listdir(root_dir)
            if os.path.isdir(os.path.join(root_dir, d))
        ]

    def __len__(self):
        return len(self.video_folders)

    def __getitem__(self, idx):
        video_folder = self.video_folders[idx]
        frame_files = sorted(os.listdir(video_folder))
        selected_frames = frame_files[:self.num_frames]

        frames = []
        for frame_file in selected_frames:
            frame_path = os.path.join(video_folder, frame_file)
            image = Image.open(frame_path).convert("RGB")
            if self.transform:
                image = self.transform(image)
            frames.append(image)

        frames_tensor = torch.stack(frames)
        label = 1 if "fi" in video_folder.lower() else 0  # 'fi' = violent, 'no' = non-violent
        return frames_tensor, torch.tensor(label)

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

from torch.utils.data import DataLoader

TEST_DIR = "/content/drive/MyDrive/data/test"  # adjust if needed

test_dataset = RPATPoseDataset(TEST_DIR, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

print("‚úÖ Number of test samples:", len(test_dataset))

import os

print("Contents of /content/drive/MyDrive/data:")
print(os.listdir("/content/drive/MyDrive/data"))

import os

TEST_DIR = "/content/drive/MyDrive/data/test"
print("‚úÖ Exists:", os.path.exists(TEST_DIR))
print("üìÅ Contents of test folder:", os.listdir(TEST_DIR)[:10])

import cv2
import os

SOURCE_DIR = "/content/drive/MyDrive/data/test"
OUTPUT_DIR = "/content/drive/MyDrive/data/test_processed"

os.makedirs(OUTPUT_DIR, exist_ok=True)

for file in os.listdir(SOURCE_DIR):
    if file.endswith(".avi"):
        video_path = os.path.join(SOURCE_DIR, file)
        video_name = os.path.splitext(file)[0]
        output_folder = os.path.join(OUTPUT_DIR, video_name)
        os.makedirs(output_folder, exist_ok=True)

        cap = cv2.VideoCapture(video_path)
        frame_count = 0
        success, frame = cap.read()
        while success:
            frame_path = os.path.join(output_folder, f"frame_{frame_count:04d}.jpg")
            cv2.imwrite(frame_path, frame)
            frame_count += 1
            success, frame = cap.read()
        cap.release()

        print(f"‚úÖ Extracted {frame_count} frames from {file}")

print("üéâ Frame extraction complete!")

TEST_DIR = "/content/drive/MyDrive/data/test_processed"

test_dataset = RPATPoseDataset(TEST_DIR, transform=transform)
print("‚úÖ Number of test samples:", len(test_dataset))

from torch.utils.data import DataLoader
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

import torch
import torch.nn as nn

class RPATTransformer(nn.Module):
    def __init__(self, input_dim=2048, num_classes=2):
        super(RPATTransformer, self).__init__()
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=input_dim, nhead=8, batch_first=True),
            num_layers=2
        )
        self.fc = nn.Linear(input_dim, num_classes)

    def forward(self, x):
        # x shape: [batch, frames, features]
        x = self.transformer(x)
        x = x.mean(dim=1)  # temporal pooling
        x = self.fc(x)
        return x

model = RPATTransformer(input_dim=2048, num_classes=2)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

import torchvision.models as models
from torchvision import transforms

# Load pretrained ResNet50 as feature extractor
feature_extractor = models.resnet50(weights="IMAGENET1K_V1")
feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])  # remove final layer
feature_extractor.eval().to(device)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np

model.eval()
all_labels = []
all_preds = []

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        B, T, C, H, W = inputs.shape
        inputs = inputs.view(B * T, C, H, W)

        with torch.no_grad():
            features = feature_extractor(inputs)
        features = features.view(B, T, -1)

        outputs = model(features)
        _, predicted = torch.max(outputs.data, 1)

        all_labels.extend(labels.cpu().numpy())
        all_preds.extend(predicted.cpu().numpy())

acc = accuracy_score(all_labels, all_preds)
print(f"‚úÖ Test Accuracy: {acc * 100:.2f}%")
print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names=["Non-Violent", "Violent"]))
print("\nConfusion Matrix:")
print(confusion_matrix(all_labels, all_preds))

import os

TRAIN_DIR = "/content/drive/MyDrive/data/train"
TEST_DIR = "/content/drive/MyDrive/data/test_processed"

def count_videos(folder):
    violent = len([d for d in os.listdir(folder) if 'fi' in d.lower()])
    non_violent = len([d for d in os.listdir(folder) if 'no' in d.lower()])
    return violent, non_violent

train_counts = count_videos(TRAIN_DIR)
test_counts = count_videos(TEST_DIR)

print(f"üß© Train Set ‚Üí Violent: {train_counts[0]}, Non-Violent: {train_counts[1]}")
print(f"üß™ Test Set ‚Üí Violent: {test_counts[0]}, Non-Violent: {test_counts[1]}")

import os
print(os.listdir("/content/drive/MyDrive/data"))

import os
print(os.listdir("/content/drive/MyDrive/data/processed"))

TRAIN_DIR = "/content/drive/MyDrive/data/processed"
TEST_DIR = "/content/drive/MyDrive/data/test_processed"

train_dataset = RPATPoseDataset(TRAIN_DIR, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)

test_dataset = RPATPoseDataset(TEST_DIR, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

print(f"‚úÖ Train samples: {len(train_dataset)}")
print(f"‚úÖ Test samples: {len(test_dataset)}")

import os
from torch.utils.data import Dataset
from PIL import Image
import torch

class RPATPoseDataset(Dataset):
    def __init__(self, root_dir, transform=None, num_frames=3):
        self.root_dir = root_dir
        self.transform = transform
        self.num_frames = num_frames

        # Collect all video folders (like fi101_xvid, no250_xvid, etc.)
        self.video_folders = [
            os.path.join(root_dir, d)
            for d in os.listdir(root_dir)
            if os.path.isdir(os.path.join(root_dir, d))
        ]

    def __len__(self):
        return len(self.video_folders)

    def __getitem__(self, idx):
        video_folder = self.video_folders[idx]
        frames_folder = os.path.join(video_folder, "frames")

        # ‚úÖ Only take actual image files (not directories)
        all_frames = [
            f for f in os.listdir(frames_folder)
            if f.lower().endswith((".jpg", ".png")) and os.path.isfile(os.path.join(frames_folder, f))
        ]
        all_frames.sort()

        # Select evenly spaced frames
        if len(all_frames) >= self.num_frames:
            step = len(all_frames) // self.num_frames
            selected_frames = [all_frames[i * step] for i in range(self.num_frames)]
        else:
            selected_frames = all_frames

        # Load and transform frames
        frames = []
        for frame_name in selected_frames:
            img_path = os.path.join(frames_folder, frame_name)
            image = Image.open(img_path).convert("RGB")
            if self.transform:
                image = self.transform(image)
            frames.append(image)

        # Stack frames
        frames = torch.stack(frames)

        # Assign label (violent = 1 if 'fi' prefix, else 0)
        label = 1 if "fi" in os.path.basename(video_folder).lower() else 0

        return frames, label

train_dataset = RPATPoseDataset(TRAIN_DIR, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)

test_dataset = RPATPoseDataset(TEST_DIR, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

print(f"‚úÖ Train samples: {len(train_dataset)}")
print(f"‚úÖ Test samples: {len(test_dataset)}")

import torch
import torch.nn as nn

class RPATTransformer(nn.Module):
    def __init__(self, input_dim=512, num_classes=2):
        super(RPATTransformer, self).__init__()
        self.encoder = nn.TransformerEncoderLayer(d_model=input_dim, nhead=8)
        self.transformer = nn.TransformerEncoder(self.encoder, num_layers=2)
        self.fc = nn.Linear(input_dim, num_classes)

    def forward(self, x):
        # x shape: [B, T, D] ‚Äî B=batch, T=frames, D=features
        if x.dim() == 4:
            # [B, T, C, H, W] ‚Üí flatten spatially if needed
            B, T, C, H, W = x.shape
            x = x.view(B, T, -1)
        x = self.transformer(x)
        x = x.mean(dim=1)
        return self.fc(x)

import torch
import torch.nn as nn
import torchvision.models as models

class RPATTransformer(nn.Module):
    def __init__(self, input_dim=512, num_classes=2):
        super(RPATTransformer, self).__init__()

        # 1Ô∏è‚É£ CNN backbone for frame-level feature extraction
        base_cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        self.cnn = nn.Sequential(*list(base_cnn.children())[:-1])  # remove fc layer
        self.feature_dim = base_cnn.fc.in_features  # usually 512

        # 2Ô∏è‚É£ Transformer for temporal modeling
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.feature_dim, nhead=8, batch_first=True)
        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=2)

        # 3Ô∏è‚É£ Classification head
        self.fc = nn.Linear(self.feature_dim, num_classes)

    def forward(self, x):
        # x: [B, T, C, H, W]
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)

        # Extract CNN features per frame
        with torch.no_grad():  # Freeze CNN during training initially
            feats = self.cnn(x).view(B, T, -1)  # [B, T, 512]

        # Transformer temporal modeling
        out = self.transformer(feats)  # [B, T, 512]
        out = out.mean(dim=1)  # average over time
        return self.fc(out)

model = RPATTransformer(input_dim=512, num_classes=2).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

transforms.Resize((112, 112))

import torch
import torch.nn as nn
import torchvision.models as models

class RPATTransformer(nn.Module):
    def __init__(self, input_dim=512, num_classes=2):
        super(RPATTransformer, self).__init__()

        # CNN backbone (ResNet18)
        base_cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        self.cnn = nn.Sequential(*list(base_cnn.children())[:-1])
        self.feature_dim = base_cnn.fc.in_features  # 512

        # Transformer
        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.feature_dim, nhead=8, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=2)

        # Final classification
        self.fc = nn.Linear(self.feature_dim, num_classes)

    def forward(self, x):
        # x shape: [B, T, C, H, W]
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)

        # Frame-level CNN features
        with torch.no_grad():  # freeze CNN for now
            feats = self.cnn(x).view(B, T, -1)  # [B, T, 512]

        # Temporal modeling
        out = self.transformer(feats)
        out = out.mean(dim=1)  # mean pooling
        return self.fc(out)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = RPATTransformer(input_dim=512, num_classes=2).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

train_loader, test_loader

from tqdm import tqdm
import torch

num_epochs = 5
model.train()

for epoch in range(num_epochs):
    running_loss, correct, total = 0.0, 0, 0

    for frames, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        frames, labels = frames.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(frames)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100 * correct / total
    print(f"‚úÖ Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%")

import os
from torch.utils.data import Dataset
from PIL import Image
from torchvision import transforms

class RPATPoseDataset(Dataset):
    def __init__(self, root_dir, transform=None, num_frames=9):
        self.root_dir = root_dir
        self.transform = transform
        self.num_frames = num_frames

        # ‚úÖ Get all video folders (each one like fi105_xvid)
        self.video_folders = [
            os.path.join(root_dir, d)
            for d in os.listdir(root_dir)
            if os.path.isdir(os.path.join(root_dir, d))
        ]

    def __len__(self):
        return len(self.video_folders)

    def __getitem__(self, idx):
        video_folder = self.video_folders[idx]

        # ‚úÖ Look for frames either in "frames" subfolder or directly in video_folder
        frames_folder = os.path.join(video_folder, "frames")
        if not os.path.exists(frames_folder):
            frames_folder = video_folder  # use folder itself if "frames/" missing

        # ‚úÖ Collect all image files
        all_frames = sorted([
            os.path.join(frames_folder, f)
            for f in os.listdir(frames_folder)
            if f.lower().endswith((".jpg", ".png"))
        ])

        if len(all_frames) == 0:
            raise FileNotFoundError(f"No image frames found in {frames_folder}")

        # ‚úÖ Sample fixed number of frames
        step = max(1, len(all_frames) // self.num_frames)
        selected_frames = all_frames[::step][:self.num_frames]

        # ‚úÖ Load and transform frames
        frames = [Image.open(f).convert("RGB") for f in selected_frames]
        if self.transform:
            frames = [self.transform(frame) for frame in frames]

        import torch
        frames_tensor = torch.stack(frames)  # Shape: [T, C, H, W]

        # ‚úÖ Assign label based on folder name
        label = 1 if "fi" in os.path.basename(video_folder).lower() else 0
        return frames_tensor, label

test_dataset = RPATPoseDataset("/content/drive/MyDrive/data/test_processed", transform=transform)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

print("‚úÖ Test samples:", len(test_dataset))

import torch
import torch.nn as nn

class RPATTransformer(nn.Module):
    def __init__(self, input_dim=512, num_classes=2):
        super(RPATTransformer, self).__init__()
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=input_dim, nhead=8, batch_first=True),
            num_layers=2
        )
        self.fc = nn.Linear(input_dim, num_classes)

    def forward(self, x):
        x = self.transformer(x)
        x = x.mean(dim=1)
        return self.fc(x)

import torch
import torch.nn as nn

class RPATTransformer(nn.Module):
    def __init__(self, input_dim=512, num_classes=2):
        super(RPATTransformer, self).__init__()
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=input_dim, nhead=8, batch_first=True),
            num_layers=2
        )
        self.fc = nn.Linear(input_dim, num_classes)

    def forward(self, x):
        # x shape ‚Üí [batch_size, sequence_length, input_dim]
        x = self.transformer(x)
        x = x.mean(dim=1)  # average pooling over time
        return self.fc(x)

import os

for root, dirs, files in os.walk("/content/drive/MyDrive"):
    for f in files:
        if f.endswith(".pth"):
            print(os.path.join(root, f))

import os

# Create directory if it doesn't exist
save_dir = "/content/drive/MyDrive/data"
os.makedirs(save_dir, exist_ok=True)
print(f"‚úÖ Directory ready: {save_dir}")

torch.save(model.state_dict(), os.path.join(save_dir, "violence_detection_model.pth"))
print("‚úÖ Model saved successfully!")

model.load_state_dict(torch.load(os.path.join(save_dir, "violence_detection_model.pth"),
                                 map_location="cuda" if torch.cuda.is_available() else "cpu"))
print("‚úÖ Model loaded successfully!")

import os

TEST_RAW = "/content/drive/MyDrive/data/test"
TEST_PROCESSED = "/content/drive/MyDrive/data/test_processed"

os.makedirs(TEST_PROCESSED, exist_ok=True)

from google.colab import drive
drive.mount('/content/drive')

import os

RAW_TEST_DIR = "/content/drive/MyDrive/data/test"
PROCESSED_TEST_DIR = "/content/drive/MyDrive/data/test_processed"

print("Raw test exists:", os.path.exists(RAW_TEST_DIR))
print("Processed test exists:", os.path.exists(PROCESSED_TEST_DIR))

import cv2

os.makedirs(PROCESSED_TEST_DIR, exist_ok=True)

for video_file in os.listdir(RAW_TEST_DIR):
    if video_file.endswith(".avi"):
        video_path = os.path.join(RAW_TEST_DIR, video_file)
        folder_name = os.path.splitext(video_file)[0]
        out_folder = os.path.join(PROCESSED_TEST_DIR, folder_name)
        os.makedirs(out_folder, exist_ok=True)

        cap = cv2.VideoCapture(video_path)
        frame_count = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                break
            cv2.imwrite(os.path.join(out_folder, f"frame_{frame_count:04d}.jpg"), frame)
            frame_count += 1
        cap.release()
        print(f"‚úÖ Extracted {frame_count} frames from {video_file}")

import torch
import torch.nn as nn

# Example RPATTransformer skeleton (adjust to your architecture)
class RPATTransformer(nn.Module):
    def __init__(self, input_dim=512, num_classes=2):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(256, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = RPATTransformer(input_dim=512, num_classes=2)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

model_path = "/content/drive/MyDrive/data/violence_detection_model.pth"
if os.path.exists(model_path):
    model.load_state_dict(torch.load(model_path, map_location=device))
    print("‚úÖ Model weights loaded successfully!")

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

class RPATPoseDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.video_folders = [
            os.path.join(root_dir, d) for d in os.listdir(root_dir)
            if os.path.isdir(os.path.join(root_dir, d))
        ]
        self.transform = transform

    def __len__(self):
        return len(self.video_folders)

    def __getitem__(self, idx):
        folder = self.video_folders[idx]
        frames_folder = os.path.join(folder, "frames")
        frame_files = sorted([
            os.path.join(frames_folder, f) for f in os.listdir(frames_folder)
            if f.endswith((".jpg", ".png"))
        ])
        # For simplicity, take first N frames or all
        images = [Image.open(f).convert("RGB") for f in frame_files[:9]]
        if self.transform:
            images = [self.transform(img) for img in images]
        # Stack frames: shape [T, C, H, W]
        video_tensor = torch.stack(images)
        label = 0 if "no" in folder else 1  # Example label logic
        return video_tensor, label

test_dataset = RPATPoseDataset(PROCESSED_TEST_DIR, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

import os
import shutil

TEST_PROCESSED = "/content/drive/MyDrive/data/test_processed"

for folder in os.listdir(TEST_PROCESSED):
    folder_path = os.path.join(TEST_PROCESSED, folder)
    if os.path.isdir(folder_path):

        frames_path = os.path.join(folder_path, "frames")

        # If frames folder does NOT exist ‚Üí create it
        if not os.path.exists(frames_path):
            os.makedirs(frames_path, exist_ok=True)

        # Move all jpg/png files into frames/
        for file in os.listdir(folder_path):
            if file.endswith((".jpg", ".png")):
                old_path = os.path.join(folder_path, file)
                new_path = os.path.join(frames_path, file)
                shutil.move(old_path, new_path)

print("‚úÖ All frames moved into frames/ subfolders!")

with torch.no_grad():
    for videos, labels in test_loader:
        print("Batch loaded successfully!")
        break

from torchvision import models

feature_extractor = models.resnet18(weights="IMAGENET1K_V1")
feature_extractor.fc = torch.nn.Identity()   # remove classification layer
feature_extractor = feature_extractor.to(device)
feature_extractor.eval()

print(model)

import torch
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

from torchvision import models

# Use ResNet18 (512-dim output)
feature_extractor = models.resnet18(weights="IMAGENET1K_V1")
feature_extractor.fc = torch.nn.Identity()
feature_extractor = feature_extractor.to(device)
feature_extractor.eval()

def extract_features(frames):
    with torch.no_grad():
        B, T, C, H, W = frames.shape
        frames = frames.view(B*T, C, H, W)
        feats = feature_extractor(frames)
        feats = feats.view(B, T, -1)
        feats = feats.mean(dim=1)
    return feats

all_preds = []
all_labels = []

with torch.no_grad():
    for videos, labels in test_loader:
        videos, labels = videos.to(device), labels.to(device)

        feats = extract_features(videos)

        outputs = model(feats)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

accuracy = (np.array(all_preds) == np.array(all_labels)).mean() * 100
print(f"‚úÖ Final Test Accuracy: {accuracy:.2f}%")
print(classification_report(all_labels, all_preds, target_names=["Non-Violent", "Violent"]))
print(confusion_matrix(all_labels, all_preds))

import torch
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
from torchvision import models

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# ResNet18 feature extractor (512-dim, matches training)
feature_extractor = models.resnet18(weights="IMAGENET1K_V1")
feature_extractor.fc = torch.nn.Identity()
feature_extractor = feature_extractor.to(device)
feature_extractor.eval()

def extract_features(frames):
    with torch.no_grad():
        B, T, C, H, W = frames.shape
        frames = frames.view(B*T, C, H, W)
        feats = feature_extractor(frames)
        feats = feats.view(B, T, -1)
        feats = feats.mean(dim=1)
    return feats

all_preds = []
all_labels = []

with torch.no_grad():
    for videos, labels in test_loader:
        videos, labels = videos.to(device), labels.to(device)

        feats = extract_features(videos)
        outputs = model(feats)

        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

accuracy = (np.array(all_preds) == np.array(all_labels)).mean() * 100
print(f"‚úÖ Final Test Accuracy: {accuracy:.2f}%")

print(classification_report(all_labels, all_preds, target_names=["Non-Violent", "Violent"]))
print(confusion_matrix(all_labels, all_preds))

torch.save(model.state_dict(), "/content/drive/MyDrive/data/violence_detection_model.pth")
print("‚úÖ Model saved")

import cv2
import torch
import numpy as np
from torchvision import transforms

transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224,224)),
    transforms.ToTensor()
])

def predict_video(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = []

    while True:
        ret, frame = cap.read()
        if not ret: break
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame = transform(frame)
        frames.append(frame)

    cap.release()

    # Convert to tensor [1, T, C, H, W]
    frames = torch.stack(frames)
    frames = frames.unsqueeze(0).to(device)

    with torch.no_grad():
        feats = extract_features(frames)
        outputs = model(feats)
        _, pred = torch.max(outputs, 1)

    return "Violent" if pred.item() == 1 else "Non-Violent"